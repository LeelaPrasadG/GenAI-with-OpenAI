{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3511a83b",
   "metadata": {},
   "source": [
    "#To install any missing packages\n",
    "\n",
    "pip install gradio\n",
    "pip show gradio\n",
    "pip install ollama\n",
    "\n",
    "# To Start Ollama\n",
    "ollama run gemma3:4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e69c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48801bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"llama3.2\"\n",
    "#MODEL = \"deepseek-r1:1.5b\"\n",
    "MODEL = \"gemma3:4b\"\n",
    "#MODEL = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b875650",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an helpful AI assistant\"\n",
    "\n",
    "#user_prompt = \"Show me the recipie for Biriyani\"\n",
    "user_prompt = \"I wish to travel to USA, Create a nice travel plan for a duration of 2 weeks. Make sure i visit all popular places of US\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afae8f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's craft an amazing 2-week US trip hitting some of the most popular spots! This plan focuses on a good balance of iconic landmarks, diverse experiences, and manageable travel times.  It's ambitious, so be prepared for some travel days.  \n",
       "\n",
       "**Important Note:** This is a *suggested* itinerary. You can customize it based on your interests (e.g., more time in nature, more focus on cities, specific activities).  Also, booking flights and accommodation *well in advance* is crucial, especially during peak season.\n",
       "\n",
       "**Theme:** Classic American Icons & City Exploration\n",
       "\n",
       "**Duration:** 14 Days / 13 Nights\n",
       "\n",
       "**Budget:** (This is highly variable) – Expect around $3,000 - $6,000+ excluding flights. This depends heavily on accommodation choices, dining, and activities.\n",
       "\n",
       "---\n",
       "\n",
       "**Week 1: East Coast & Mid-Atlantic**\n",
       "\n",
       "* **Days 1-3: New York City, New York:** (3 Nights)\n",
       "    * **Activities:** Times Square, Central Park, Statue of Liberty & Ellis Island, 9/11 Memorial & Museum, Metropolitan Museum of Art, Broadway show.\n",
       "    * **Food:** Pizza, bagels, diverse ethnic cuisine.\n",
       "* **Days 4-5: Washington D.C.:** (2 Nights)\n",
       "    * **Activities:** Smithsonian Museums (National Air and Space Museum, National Museum of Natural History), White House, Capitol Building, Lincoln Memorial, National Mall.\n",
       "    * **Food:**  Explore diverse food halls and restaurants.\n",
       "* **Days 6-7: Philadelphia, Pennsylvania:** (2 Nights)\n",
       "    * **Activities:** Independence Hall, Liberty Bell, Reading Terminal Market, Philadelphia Museum of Art (“Rocky” steps!), explore Old City.\n",
       "    * **Food:** Cheesesteaks (Pat's and Geno's), Amish specialties.\n",
       "\n",
       "\n",
       "**Week 2: Midwest & West Coast**\n",
       "\n",
       "* **Days 8-9: Chicago, Illinois:** (2 Nights)\n",
       "    * **Activities:** Millennium Park (The Bean), Art Institute of Chicago, Magnificent Mile (shopping), Navy Pier, architectural boat tour.\n",
       "    * **Food:** Deep-dish pizza, Chicago-style hot dogs.\n",
       "* **Days 10-12: Las Vegas, Nevada:** (3 Nights)\n",
       "    * **Activities:** The Strip, Bellagio fountains, casinos, shows, High Roller Observation Wheel, explore Downtown Las Vegas. \n",
       "    * **Food:** Endless dining options – from buffets to fine dining.\n",
       "* **Days 13-14: Los Angeles, California:** (2 Nights)\n",
       "    * **Activities:** Hollywood Walk of Fame, Universal Studios Hollywood, Griffith Observatory, Santa Monica Pier, Beaches. \n",
       "    * **Food:**  Tacos, diverse Californian cuisine.\n",
       "\n",
       "---\n",
       "\n",
       "**Transportation:**\n",
       "\n",
       "* **Flights:** Book round-trip flights to New York (JFK or Newark) and your departure flight from Los Angeles (LAX).\n",
       "* **Within the US:** Consider a multi-city flight or train travel between cities to save time. Amtrak is a good option for routes like Chicago to Los Angeles.  Renting a car can be useful in California and Las Vegas, but parking can be expensive.\n",
       "\n",
       "**Accommodation:**\n",
       "\n",
       "* **Variety:** Mix of hotels, Airbnb, or hostels depending on your budget and preferences. Book early for best prices.\n",
       "\n",
       "**Tips for Planning:**\n",
       "\n",
       "* **National Parks:**  If you're interested in National Parks (Grand Canyon, Yellowstone, etc.), this itinerary doesn't include them. You’d need to add several days (potentially a week) to incorporate those.\n",
       "* **Weather:** Research the weather for each location during your travel dates.\n",
       "* **Visa:** Check visa requirements based on your nationality.\n",
       "* **Currency:** US Dollar (USD)\n",
       "* **Tipping:** Tipping is customary in the US (15-20% for good service).\n",
       "\n",
       "---\n",
       "\n",
       "**To help me refine this itinerary and make it even better for *you*, could you tell me:**\n",
       "\n",
       "*   **What are your main interests?** (e.g., History, Art, Nature, Food, Theme Parks, Music, etc.)\n",
       "*   **What is your approximate budget for this trip?** (e.g., Budget, Mid-Range, Luxury)\n",
       "*   **Who are you travelling with?** (Solo, couple, family with children – ages?)\n",
       "*   **Are there any specific places you *definitely* want to see?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model = MODEL,\n",
    "    messages =[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    ")\n",
    "#print(response)\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff0cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9c8f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed807208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://364eefba4b8b9ff224.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://364eefba4b8b9ff224.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input Which cricket team can be termed as Best?\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394c70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://d25a01fb6c3e54fd6e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d25a01fb6c3e54fd6e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input Which cricket team can be termed as Best?\n"
     ]
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67e770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt):\n",
    "    response = ollama.chat(\n",
    "    model = MODEL,\n",
    "    messages =[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb65ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://0fa6592555162ade7f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0fa6592555162ade7f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25ae5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "def stream_gpt(prompt):\n",
    "    response = ollama.chat(\n",
    "    model = MODEL,\n",
    "    stream= True,\n",
    "    messages =[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.message.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ef290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://b2e8c277b68f24792f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b2e8c277b68f24792f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3815b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"Llama\":\n",
    "        MODEL = \"llama3.2:1b\"\n",
    "    elif model==\"Gemma\":\n",
    "        MODEL = \"gemma3:1b\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    \n",
    "    response = ollama.chat(\n",
    "    model = MODEL,\n",
    "    stream= True,\n",
    "    messages =[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.message.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc54da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* Running on public URL: https://94791bcba07fcbf76f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://94791bcba07fcbf76f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 278, in __call__\n",
      "    await wrap(partial(self.listen_for_disconnect, receive))\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 274, in wrap\n",
      "    await func()\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 242, in listen_for_disconnect\n",
      "    message = await receive()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 531, in receive\n",
      "    await self.message_event.wait()\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\locks.py\", line 212, in wait\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError: Cancelled by cancel scope 1eefb010920\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_utils.py\", line 77, in collapse_excgroups\n",
      "  |     yield\n",
      "  |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 271, in __call__\n",
      "  |     async with anyio.create_task_group() as task_group:\n",
      "  |   File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 678, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    |     result = await app(  # type: ignore[func-returns-value]\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    |     return await self.app(scope, receive, send)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    |     await super().__call__(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    |     await self.middleware_stack(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    |     raise exc\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    |     await self.app(scope, receive, _send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    |     return await self.app(scope, receive, send)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\route_utils.py\", line 878, in __call__\n",
      "    |     await self.app(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    |     raise exc\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    |     await app(scope, receive, sender)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    |     await self.middleware_stack(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    |     await route.handle(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    |     await self.app(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 78, in app\n",
      "    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    |     raise exc\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    |     await app(scope, receive, sender)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    |     await response(scope, receive, send)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 270, in __call__\n",
      "    |     with collapse_excgroups():\n",
      "    |   File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_utils.py\", line 83, in collapse_excgroups\n",
      "    |     raise exc\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 274, in wrap\n",
      "    |     await func()\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 254, in stream_response\n",
      "    |     async for chunk in self.body_iterator:\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\routes.py\", line 1492, in sse_stream\n",
      "    |     raise e\n",
      "    |   File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\routes.py\", line 1464, in sse_stream\n",
      "    |     ].remove(message.event_id)\n",
      "    |       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    | KeyError: 'c20b7fb503834f8da843e9abf79c6ed6'\n",
      "    +------------------------------------\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\route_utils.py\", line 878, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 78, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\routing.py\", line 76, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 270, in __call__\n",
      "    with collapse_excgroups():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\_utils.py\", line 83, in collapse_excgroups\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 274, in wrap\n",
      "    await func()\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\starlette\\responses.py\", line 254, in stream_response\n",
      "    async for chunk in self.body_iterator:\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\routes.py\", line 1492, in sse_stream\n",
      "    raise e\n",
      "  File \"C:\\Users\\Babji\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\routes.py\", line 1464, in sse_stream\n",
      "    ].remove(message.event_id)\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'c20b7fb503834f8da843e9abf79c6ed6'\n"
     ]
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"), gr.Dropdown([\"Llama\", \"Gemma\"], label=\"Select model\", value=\"Gemma\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fb105d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(product_description):\n",
    "    return f\"\"\"\n",
    "You are a pricing expert. Based on the product description below, predict a fair market price in USD.\n",
    "Consider factors like brand, specifications, features, and comparable products. Provide exact estimate of the price instead of a price range if possible.\n",
    "\n",
    "Product Description:\n",
    "\\\"\\\"\\\"\n",
    "{product_description}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Output:\n",
    "Price estimate (in USD) with brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccc596fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(product_description,model):\n",
    "    prompt = generate_prompt(product_description)\n",
    "\n",
    "    if model==\"Llama\":\n",
    "        MODEL = \"llama3.2:1b\"\n",
    "    elif model==\"Gemma\":\n",
    "        MODEL = \"gemma3:1b\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    \n",
    "    system_prompt = \"You are a helpful retail assistant which will predict the exact price of the product in USD\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "    model = MODEL,\n",
    "    stream= True,\n",
    "    messages =[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in response:\n",
    "        result += chunk.message.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f0b035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"), gr.Dropdown([\"Llama\", \"Gemma\"], label=\"Select model\", value=\"Gemma\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
